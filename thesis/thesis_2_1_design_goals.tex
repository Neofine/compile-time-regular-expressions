\section{Design Goals and Constraints}

This section articulates the design objectives that guided the development of the SIMD optimization framework for CTRE, along with the technical and architectural constraints that bounded the solution space.

\subsection{Primary Design Goals}

\subsubsection{Comprehensive Pattern Coverage}

The optimization framework targets all regular expression pattern classes amenable to data-parallel execution. The selection of optimization targets is based on the inherent parallelizability of each pattern type rather than empirical usage frequencies. Character class repetitions receive primary focus due to their exhibition of maximal data-level parallelism: each character comparison operation is independent, memory access patterns are sequential, and the comparison predicate can be expressed in a constant number of SIMD instructions irrespective of input size.

The theoretical parallelism factor for character class matching on AVX2 architectures is 32:1, corresponding to 32 bytes processed per vector instruction versus single-byte scalar processing. This represents the upper bound for SIMD optimization in character-oriented pattern matching. Alternation patterns and literal strings present distinct technical challenges due to control flow divergence and compile-time string embedding in CTRE's architecture, respectively. Complex patterns involving backreferences or assertions are explicitly excluded from SIMD optimization due to fundamental data dependencies that preclude vectorization.

\subsubsection{Zero-Overhead Abstraction}

The design adheres strictly to the zero-overhead principle: pattern types unsuitable for SIMD acceleration must experience no performance degradation relative to the scalar baseline. This requirement precludes uniform dispatch mechanisms that impose overhead across all pattern types. The implementation leverages CTRE's compile-time pattern representation to perform optimization strategy selection at compile time, enabling aggressive dead code elimination. Patterns determined at compile time to be SIMD-incompatible generate no vectorization-related instructions in the compiled binary.

Scalar fallback paths—required for SIMD-unsuitable patterns and for processing input remainders after vector operations—must maintain performance parity with hand-optimized scalar implementations. Performance regression in scalar paths would fundamentally compromise the design, as a substantial fraction of patterns and input sizes necessitate scalar processing.

\subsubsection{Compile-Time Optimization Selection}

CTRE's distinguishing characteristic is compile-time pattern compilation, wherein regular expression patterns are transformed into C++ type representations during compilation. This architectural property enables optimization strategy selection to occur at compile time through template metaprogramming, eliminating runtime pattern analysis overhead entirely. Pattern characteristics—including character class density, negation semantics, and repetition bounds—are computed as compile-time constants via template specialization and \texttt{constexpr} evaluation. The compiler subsequently eliminates inapplicable code paths through standard dead code elimination passes.

\subsubsection{Portability Across Instruction Set Architectures}

The implementation must function correctly across the spectrum of x86-64 SIMD capabilities: AVX2 (256-bit vectors), SSE4.2 (128-bit vectors), and scalar (no SIMD). The design employs a hybrid approach combining runtime capability detection with compile-time optimization path generation. SIMD capability is detected once at program initialization via CPUID instructions and cached in thread-local storage to amortize detection overhead. Each optimization path is compiled conditionally, with the runtime system selecting the appropriate implementation based on detected capabilities.

AVX-512 is explicitly excluded from the target instruction set despite its potential for doubled throughput. This exclusion is motivated by three factors: limited deployment on client-class processors, documented frequency throttling that can negate throughput advantages~\cite{intel-avx512-freq}, and substantially increased implementation complexity due to 512-bit vector width and mask register management.

\subsection{Architectural Constraints}

\subsubsection{Preservation of Compile-Time Semantics}

CTRE represents regular expression patterns as nested C++ template types, with pattern compilation occurring entirely at compile time. The pattern \texttt{[a-z]+}, for instance, is represented as \texttt{repeat<1, unlimited, set<char\_range<'a', 'z'>>>}. This type-based representation is fundamental to CTRE's design and must be preserved by any optimization framework.

The optimization logic must consequently operate on type-level pattern representations, extracting pattern characteristics through template specialization, SFINAE (Substitution Failure Is Not An Error), and C++20 \texttt{requires} clauses. Runtime pattern analysis is architecturally infeasible, as patterns do not exist as runtime data structures. This constraint necessitates a purely compile-time approach to optimization strategy selection.

\subsubsection{Correctness Preservation}

SIMD implementations must produce results bit-identical to scalar implementations across all edge cases and input configurations. Specific correctness requirements include proper handling of: high-bit characters (Unicode range 0x80-0xFF), negated character classes, case-insensitive matching semantics, zero-length matches, and input boundary conditions.

During development, a signed integer overflow bug was identified in character range calculations: the expression \texttt{max\_char - min\_char + 1} exhibited undefined behavior when \texttt{max\_char} and \texttt{min\_char} were represented as signed 8-bit integers. The correction involved casting operands to unsigned 8-bit integers prior to arithmetic operations, ensuring defined wrapping semantics for the range calculation.

\subsubsection{Instruction Cache Pressure}

SIMD implementations generate larger code footprints than scalar equivalents due to vector instruction encoding, multiple instruction set paths, and loop unrolling. Aggressive function inlining, while theoretically beneficial for eliminating call overhead, was empirically observed to cause instruction cache thrashing when total code size exceeded L1 instruction cache capacity (typically 32KB on contemporary x86-64 processors). The design consequently delegates inlining decisions to the compiler's optimization passes rather than mandating inline expansion via attributes.

\subsubsection{Input Size Threshold Selection}

SIMD operations exhibit fixed overhead comprising: capability detection (amortized to near-zero through caching), vector register manipulation, and mask generation. For sufficiently short inputs, this overhead dominates actual computation time, rendering SIMD optimization counterproductive. Empirical measurement across pattern types and input sizes identified two distinct thresholds: simple single-character and dense-range patterns benefit from SIMD at 16-byte inputs, while complex patterns including negation require 28-byte inputs to amortize overhead costs.

The threshold selection is implemented as a compile-time constant computed from pattern characteristics:
\begin{verbatim}
constexpr size_t threshold =
    (range_size <= 2 && !is_negated) ? 16 : 28;
\end{verbatim}
This pattern-aware threshold prevents performance degradation on short inputs while enabling SIMD acceleration for inputs of sufficient length.

\subsection{Explicit Non-Goals}

\subsubsection{Search Operation Optimization}

The design scope is explicitly limited to match operations (exact pattern matching at a specific input position) rather than search operations (locating pattern occurrences within input text). Match operations are the primary operation in CTRE's use cases, which center on validation and parsing rather than full-text search. Search optimization would require fundamentally different algorithmic approaches such as Boyer-Moore preprocessing or Teddy-style multi-string matching~\cite{hyperscan-teddy}.

An exploratory implementation of the Teddy algorithm was developed to assess its applicability to match operations. While Teddy demonstrated substantial performance improvements for search operations (geometric mean speedup of 18.4× across varying input sizes), it provided minimal benefit for match operations (1.5-1.7× speedup) at a cost of 1150 additional lines of code. Based on cost-benefit analysis, the Teddy implementation was rejected in favor of simpler threshold tuning that achieved comparable match performance with negligible code complexity.

\subsubsection{Complex Pattern Optimization}

Patterns involving backreferences, lookahead or lookbehind assertions, conditional expressions, and recursive structures are explicitly excluded from optimization scope. These pattern types exhibit control flow dependencies and non-local information requirements that fundamentally preclude data-parallel execution. Furthermore, empirical analysis of production regex corpuses suggests such patterns constitute less than 5\% of practical regular expression usage while requiring disproportionate implementation complexity.

\subsubsection{AVX-512 Instruction Set Support}

Despite AVX-512's theoretical doubling of vector throughput relative to AVX2, this instruction set is excluded from the implementation. The exclusion is justified by three considerations: (1) limited deployment on mainstream consumer processors, restricting the beneficiary population; (2) frequency throttling behavior where AVX-512 execution triggers dynamic voltage and frequency scaling (DVFS) that reduces processor frequency, potentially negating throughput gains; (3) substantially increased implementation complexity due to 512-bit vector manipulation and 64-bit mask register management compared to AVX2's 32-bit masks.
