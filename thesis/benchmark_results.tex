\section{Experimental Evaluation}
\label{sec:evaluation}

This section presents empirical measurements comparing CTRE-SIMD against the baseline CTRE implementation and four established regex engines: RE2~\cite{re2} (Google's production engine), PCRE2~\cite{pcre2} (Perl-compatible reference implementation), Hyperscan~\cite{hyperscan} (Intel's SIMD-optimized engine), and \texttt{std::regex} (C++ standard library). All measurements use full-string matching on 1000 inputs per size (200 for \texttt{std::regex} due to performance constraints), reporting mean time per match across 10 iterations following 3 warmup runs. Heatmaps display speedup relative to baseline CTRE at 1024-byte inputs unless otherwise noted.

\subsection{Simple Character Classes}
\label{sec:eval-simple}

Single character class repetitions represent the most favorable case for SIMD vectorization---uniform operations applied across contiguous memory. Figure~\ref{fig:simple-heatmap} summarizes performance across five fundamental patterns.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/simple/heatmap.png}
\caption{Speedup heatmap for simple character class patterns at 1024 bytes. CTRE-SIMD achieves 25$\times$ speedup over baseline CTRE for contiguous ranges. Hyperscan demonstrates competitive performance (2.5$\times$ faster than baseline CTRE) through its own SIMD infrastructure.}
\label{fig:simple-heatmap}
\end{figure}

The 25$\times$ speedup for \texttt{[0-9]+} derives from AVX2's ability to test 32 characters per instruction against range bounds, versus scalar code's single character per iteration. This represents near-optimal utilization of the 256-bit register width.

Figure~\ref{fig:digits-scaling} reveals how speedup varies with input size---a critical consideration for practical deployment.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/simple/digits_time.png}
\caption{Matching time versus input size for \texttt{[0-9]+}. At 16 bytes, CTRE-SIMD exhibits no advantage due to setup overhead. Speedup emerges at 32+ bytes and stabilizes around 25$\times$ for larger inputs.}
\label{fig:digits-scaling}
\end{figure}

\paragraph{Limitation: Small inputs.} At 16-byte inputs, CTRE-SIMD provides no measurable benefit. Register initialization, mask generation, and result extraction introduce fixed overhead that dominates when processing fewer than 32 bytes. Applications with predominantly small strings should consider disabling SIMD via the \texttt{CTRE\_DISABLE\_SIMD} preprocessor flag.

\subsection{Complex Structured Patterns}
\label{sec:eval-complex}

Production patterns typically combine character classes with literal sequences and structural elements. Figure~\ref{fig:complex-heatmap} evaluates patterns representative of configuration files, URLs, and log formats.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/complex/heatmap.png}
\caption{Speedup for complex patterns at 1024 bytes. Hexadecimal (\texttt{[0-9a-fA-F]+}) achieves 25$\times$ via multi-range SIMD. Patterns with significant literal prefixes (\texttt{http://[a-z]+}) show reduced but substantial gains of 8$\times$.}
\label{fig:complex-heatmap}
\end{figure}

The hexadecimal pattern \texttt{[0-9a-fA-F]+} demonstrates that multi-range character classes vectorize effectively---three disjoint ranges evaluated in parallel rather than sequentially. However, the \texttt{decimal} pattern (\texttt{[0-9]+\textbackslash.[0-9]+}) shows only 8$\times$ speedup because the literal dot character forces a scalar transition mid-pattern.

Figure~\ref{fig:complex-time} illustrates scaling behavior for a representative pattern combining character classes with literals.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/complex/letters_digits_time.png}
\caption{Time scaling for \texttt{[a-z]+[0-9]+}. CTRE-SIMD maintains lower latency than all runtime engines across the measured range. Note: times below 10ns approach timer resolution and exhibit measurement variance.}
\label{fig:complex-time}
\end{figure}

\subsection{Pattern Scaling: Alternations versus Character Classes}
\label{sec:eval-scaling}

A non-obvious implementation achievement is treating alternations like \texttt{(a|b)+} equivalently to character classes \texttt{[ab]+}. Naively, alternations would require separate NFA branches with backtracking; our compile-time analysis recognizes single-character alternations and generates unified SIMD code. Figure~\ref{fig:scaling-heatmap} validates this optimization.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/scaling/heatmap.png}
\caption{Alternation versus character class performance at 1024 bytes. Both \texttt{(a|b)+} and \texttt{[ab]+} achieve 25$\times$ speedup, confirming compile-time normalization. PCRE2 shows dramatically worse performance on alternations (0.01$\times$ vs 0.07$\times$) due to backtracking overhead.}
\label{fig:scaling-heatmap}
\end{figure}

Notably, \texttt{std::regex} performance degrades catastrophically on alternations---80ms per match at 1024 bytes for \texttt{(a|b|c|d)+} versus 30ms for the equivalent \texttt{[abcd]+}. This confirms that compile-time pattern normalization provides benefits beyond SIMD enablement.

\subsection{Non-Matching Input Rejection}
\label{sec:eval-nomatch}

Many production workloads involve filtering: most inputs fail validation. The dominator and region analysis described in Section~\ref{sec:prefilter} enables early rejection without full pattern evaluation. Figure~\ref{fig:nomatch-heatmap} presents results on inputs guaranteed to fail matching.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/nomatch/heatmap.png}
\caption{Non-matching input rejection speedups at 1024 bytes. Patterns with extractable literals (\texttt{dom\_suffix}: \texttt{[a-z]+test}) show 178$\times$ speedup over baseline CTRE through SIMD prefiltering. Patterns where literals appear at input start (\texttt{dom\_prefix}: \texttt{test[a-z]+}) reject in the first character check, achieving near-instant rejection regardless of SIMD.}
\label{fig:nomatch-heatmap}
\end{figure}

\paragraph{Mechanism.} For pattern \texttt{[a-z]+test} on input containing only letters a-f (no `t'), baseline CTRE must scan the entire 1024-byte string character-by-character, hoping to encounter the literal ``test''. CTRE-SIMD's prefilter uses vectorized literal search to confirm absence in approximately 18ns versus 3178ns---a 178$\times$ improvement.

\paragraph{Limitation: prefix patterns.} When the required literal appears at position zero (\texttt{dom\_prefix}: \texttt{test[a-z]+}), the first-character check in both baseline and SIMD immediately rejects non-matching input. SIMD provides no advantage (speedup $\approx$1.0$\times$) because rejection occurs before any vectorized code executes.

\subsection{Real-World Data Formats}
\label{sec:eval-realworld}

To assess practical relevance, we evaluate patterns for common validation tasks: IPv4 addresses, UUIDs, email addresses, and ISO dates. Figure~\ref{fig:realworld-heatmap} presents results.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/realworld/heatmap.png}
\caption{Validation pattern speedups at 1024 bytes. UUID (\texttt{[0-9a-f]+(-[0-9a-f]+)\{4\}}) achieves 18$\times$ through parallel hex-range evaluation. IPv4 shows 11$\times$ improvement. All patterns outperform runtime engines by substantial margins.}
\label{fig:realworld-heatmap}
\end{figure}

The UUID pattern benefits from five repeated hex-digit groups, each vectorized independently. IPv4's improvement is more modest because the pattern requires four separate digit groups with literal dot separators, limiting contiguous SIMD application.

\subsection{Input Size Extremes}
\label{sec:eval-size}

SIMD effectiveness depends critically on input size. We explicitly benchmark extreme cases to characterize the technique's operational envelope.

\paragraph{Small inputs (64 bytes).} At 64 bytes, SIMD achieves 15$\times$ speedup for simple patterns---sufficient to justify the optimization despite partial register utilization. Below 32 bytes, returns diminish significantly.

\paragraph{Large inputs (64KB).} At 65536 bytes, CTRE-SIMD maintains 26$\times$ speedup, demonstrating that vectorization scales linearly with input size. Memory bandwidth rather than computation becomes the bottleneck for larger inputs, though this threshold was not reached in our measurements.

\subsection{Advanced Pattern Features}
\label{sec:eval-fallback}

Not all regex features admit straightforward SIMD acceleration. Figure~\ref{fig:fallback-heatmap} presents patterns exercising backreferences, non-greedy quantifiers, and lookahead assertions---features often assumed to preclude vectorization.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/fallback/heatmap.png}
\caption{Performance on advanced pattern features at 1024 bytes. Single-character backreferences (\texttt{(.)\textbackslash 1+}, \texttt{((.)}\texttt{\textbackslash 2)+}) achieve 1.2--1.3$\times$ speedup via broadcast-compare optimization. Truly ineligible patterns (lazy quantifiers, lookaheads, group repetition) show speedups near 1.0$\times$, confirming correct fallback to scalar execution.}
\label{fig:fallback-heatmap}
\end{figure}

\paragraph{Single-character backreferences: a surprising optimization.} Patterns like \texttt{(.)\textbackslash 1+} appear to require sequential processing---the backreference content is unknown until runtime. However, when the captured group matches exactly one character, CTRE-SIMD applies a broadcast-compare optimization:

\begin{enumerate}
    \item Capture the single character into a scalar register
    \item Broadcast it to all 32 lanes using \texttt{vpbroadcastb}
    \item Compare 32 consecutive characters using \texttt{vpcmpeqb}
\end{enumerate}

This yields 1.2--1.3$\times$ speedup for patterns like \texttt{(.)\textbackslash 1+} (matching runs of repeated characters such as ``aaaa'') and \texttt{((.)}\texttt{\textbackslash 2)+} (nested single-character backreferences). The optimization applies only when the backreference targets a single-character capture; multi-character captures like \texttt{([a-z]+)\_\textbackslash 1} cannot be vectorized for the backreference portion, though the \texttt{[a-z]+} prefix still benefits from SIMD range checking.

\paragraph{Truly ineligible features.} The following constructs cannot benefit from SIMD and correctly fall back to scalar execution:

\begin{itemize}
    \item \textbf{Non-greedy quantifiers} (\texttt{[a-z]*?x}): Require backtracking semantics---the engine must try the shortest match first, then progressively extend. Forward-only SIMD scanning cannot express this behavior.
    \item \textbf{Lookahead assertions} (\texttt{[a-z](?=[0-9])}): Require position-dependent context evaluation that consumes no input. The assertion must be checked at each potential match position, precluding batch processing.
    \item \textbf{Capturing groups in repetitions} (\texttt{(abc)+}): Each iteration must record separate capture boundaries, requiring per-iteration bookkeeping incompatible with SIMD's uniform lane processing.
\end{itemize}

Critically, compile-time pattern analysis correctly identifies these cases and generates \emph{only} scalar code---no SIMD infrastructure is instantiated, resulting in zero runtime overhead and zero code bloat for ineligible patterns.

\subsection{Compilation Overhead}
\label{sec:eval-compile}

Template metaprogramming incurs compile-time costs. Figure~\ref{fig:compile-time} quantifies this overhead.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/compile_time/compile_time_by_category.png}
\caption{Compilation time by pattern category. Simple patterns add 2--5\% overhead (25ms). Complex patterns with multiple groups (UUID) incur up to 200\% overhead (2.2 seconds additional).}
\label{fig:compile-time}
\end{figure}

For simple patterns, SIMD adds approximately 25ms to a 900ms baseline---negligible in most build systems. Complex patterns with multiple character classes and repetitions can increase compilation time substantially. The UUID pattern (\texttt{[0-9a-f]+(-[0-9a-f]+)\{4\}}) requires 3.3 seconds with SIMD versus 1.1 seconds baseline.

\paragraph{Mitigation.} For patterns in hot paths matching millions of inputs, even seconds of additional compile time amortizes to microseconds per match. For patterns matched infrequently, the \texttt{CTRE\_DISABLE\_SIMD} flag eliminates the overhead.

\subsection{Code Size Impact}
\label{sec:eval-codesize}

SIMD template instantiation increases binary size for eligible patterns. Figure~\ref{fig:codesize} presents measurements of the \texttt{.text} section for compiled benchmark executables.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/codesize/codesize_comparison.png}
\caption{Binary code size comparison. SIMD-eligible patterns (digits, lowercase, alternations) add 1--6KB to the \texttt{.text} section. SIMD-ineligible patterns (lazy\_star, lookahead, group\_repeat) show \textbf{zero overhead}---identical binary size with and without SIMD enabled.}
\label{fig:codesize}
\end{figure}

The size increase for eligible patterns stems from SIMD intrinsic expansion and template instantiations for SSE/AVX2 code paths. Patterns that cannot benefit from SIMD (non-greedy quantifiers, lookaheads, capturing group repetitions) incur \textbf{no code size penalty}---compile-time analysis determines ineligibility before any SIMD templates are instantiated. This zero-overhead guarantee is essential for embedded systems where code size constraints prohibit speculative code generation.

\subsection{Comparison with Runtime Engines}
\label{sec:eval-comparison}

Table~\ref{tab:engine-comparison} summarizes CTRE-SIMD performance relative to runtime engines at 1024 bytes for \texttt{[0-9]+}.

\begin{table}[htbp]
\centering
\caption{Engine comparison for \texttt{[0-9]+} at 1024 bytes}
\label{tab:engine-comparison}
\begin{tabular}{lrrl}
\hline
\textbf{Engine} & \textbf{Time (ns)} & \textbf{Speedup vs CTRE-SIMD} & \textbf{Notes} \\
\hline
CTRE-SIMD & 17.5 & 1.0$\times$ & --- \\
Hyperscan & 45.0 & 0.39$\times$ & SIMD-optimized \\
CTRE (baseline) & 445.6 & 0.039$\times$ & Scalar \\
RE2 & 1557.8 & 0.011$\times$ & DFA-based \\
PCRE2 & 6624.4 & 0.0026$\times$ & NFA backtracking \\
\texttt{std::regex} & 27886.3 & 0.0006$\times$ & Recursive backtracking \\
\hline
\end{tabular}
\end{table}

Hyperscan, Intel's purpose-built SIMD regex engine, achieves only 2.6$\times$ slower performance than CTRE-SIMD despite extensive optimization. This suggests that compile-time pattern specialization provides advantages beyond what runtime JIT compilation can achieve. However, Hyperscan supports features (streaming matching, simultaneous multi-pattern) that CTRE does not attempt.

\subsection{Threats to Validity}
\label{sec:eval-threats}

\paragraph{Internal validity.} Measurements were taken on a single machine configuration (AMD Zen architecture with AVX2). Results may differ on Intel processors or systems with only SSE4.2 support. Timer resolution limits (approximately 10ns) introduce noise for fast operations.

\paragraph{External validity.} Benchmark patterns, while representative, do not exhaustively cover all regex use cases. Production workloads with complex Unicode handling, multi-character backreferences, or streaming requirements may exhibit different performance characteristics.

\paragraph{Construct validity.} Full-string matching semantics differ from substring search workloads common in log analysis. Substring search would favor Hyperscan's streaming architecture over our single-match approach.

\subsection{Summary}
\label{sec:eval-summary}

The experimental evaluation demonstrates:

\begin{enumerate}
    \item \textbf{Substantial speedups for eligible patterns}: 25$\times$ for simple character classes, 8--25$\times$ for complex patterns, scaling consistently from 64 bytes to 64KB inputs.

    \item \textbf{Early rejection benefits}: 178$\times$ speedup for non-matching inputs when dominator analysis extracts usable literals.

    \item \textbf{Intelligent partial optimization}: Single-character backreferences achieve 1.2--1.3$\times$ speedup via broadcast-compare, while multi-character backreferences still benefit from SIMD on their character class components.

    \item \textbf{True zero-overhead fallback}: Patterns with non-greedy quantifiers, lookaheads, and group repetitions incur no compile-time, runtime, or code-size penalty---compile-time analysis generates only scalar code.

    \item \textbf{Compilation costs}: 2--200\% compile time overhead and 0--55\% code size increase, depending on pattern eligibility and complexity.

    \item \textbf{Competitive with specialized engines}: 2.6$\times$ faster than Hyperscan on simple patterns despite Hyperscan's dedicated SIMD infrastructure.
\end{enumerate}

The technique is most beneficial for applications matching large volumes of medium-to-large strings against character-class-heavy patterns. It gracefully degrades for ineligible patterns without penalizing overall application performance, and opportunistically optimizes unexpected cases like single-character backreferences.
