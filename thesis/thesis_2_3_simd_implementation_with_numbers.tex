\subsection{SIMD-Accelerated Matching Model}
\label{sec:simd-model}

This section describes the vectorised matching strategy and its integration into CTRE's compile-time evaluation framework. The implementation employs compile-time pattern analysis to select appropriate execution paths, maintaining zero overhead for patterns unsuitable for vectorisation.

\subsubsection{Pattern Classification and Dispatch}

The matching pipeline performs compile-time pattern analysis to determine the optimal execution strategy:

\begin{itemize}
\item \textbf{Character class analysis}: Range patterns \texttt{[a-z]} activate SIMD comparison paths (primary optimization)
\item \textbf{Literal extraction}: Glushkov NFA analysis identifies required substrings for prefiltering
\item \textbf{Repetition detection}: Quantified patterns \texttt{a+}, \texttt{[0-9]*} enable SIMD bulk processing
\item \textbf{Fallback}: Complex patterns (backreferences, lookahead, alternations) use template evaluation
\end{itemize}

Classification occurs via \texttt{if constexpr}, ensuring unused paths incur no runtime overhead. Assembly verification confirms identical code generation for patterns with and without SIMD analysis headers included.

\subsubsection{Literal Extraction via Graph Analysis}

The implementation constructs a Glushkov NFA from the pattern AST at compile-time, then applies graph algorithms to extract required literals for prefiltering.

\paragraph{Glushkov NFA Construction}
Each position in the regex corresponds to an NFA state. The construction computes:
\begin{itemize}
\item \texttt{FIRST(P)}: States reachable at start
\item \texttt{LAST(P)}: States reachable at end
\item \texttt{FOLLOW(P, i)}: States reachable after position $i$
\end{itemize}

The resulting NFA is represented as a \texttt{constexpr} graph structure with compile-time accessible transitions and accept states.

\paragraph{Dominator Analysis}
A vertex $v$ in the NFA is a \emph{dominator} if all paths from start to accept states traverse $v$. The algorithm:

\begin{enumerate}
\item For each state $v$ (excluding start):
\item Compute reachability from start to accept with $v$ removed
\item If no accept state reachable: $v$ is a dominator
\end{enumerate}

Consecutive dominator positions with concrete characters form extractable literals. For pattern \texttt{(foo|bar)test}, positions corresponding to ``test'' are dominators, yielding a 4-character literal.

\paragraph{Region Analysis (Fallback)}
When dominator analysis produces no literals (e.g., \texttt{(fooX|barX|bazX)}), region analysis partitions the NFA into alternation branches and extracts common suffixes. Combined, these provide 97\%+ pattern coverage.

\paragraph{Integration}
Extracted literals enable fail-fast rejection:

\begin{verbatim}
if constexpr (has_prefilter_literal<RE>) {
    constexpr auto literal = prefilter_literal<RE>;
    
    if constexpr (literal.length >= 2) {
        bool found = scan_for_literal(begin, end, literal);
        if (!found) return not_matched;  // Fail-fast
    }
}
\end{verbatim}

All analysis occurs at compile-time. Patterns without extractable literals skip the check via \texttt{if constexpr} with zero runtime cost. Empirical testing showed literal prefiltering provides neutral to slight gains on small inputs (16-512 bytes), with greater potential on larger inputs where scan overhead amortizes.

\subsubsection{BitNFA Evaluation: Implementation and Trade-offs}

Alternation patterns \texttt{(A|B|C)} were hypothesized to benefit from bit-parallel NFA representation inspired by Hyperscan~\cite{wang2019hyperscan}. A complete BitNFA implementation was developed and evaluated.

\paragraph{State Representation}
Active states stored as 128-bit bitmasks:
\begin{itemize}
\item \texttt{current\_states}: Bitmask of active states
\item \texttt{accept\_mask}: Bitmask of accept states
\item Match succeeds if \texttt{current\_states \& accept\_mask != 0}
\end{itemize}

\paragraph{Transition Computation}
For each input character $c$:
\begin{verbatim}
next_states = 0
for each active state s:
    next_states |= transition_table[s][c]
current_states = next_states
\end{verbatim}

The NFA is compiled at compile-time via \texttt{constexpr}, eliminating construction overhead.

\paragraph{Empirical Evaluation}
Systematic benchmarking revealed BitNFA unsuitable for CTRE's workload:

\begin{itemize}
\item \textbf{Micro-benchmark}: 12.05ns vs 1.35ns for base CTRE (8.9$\times$ slower)
\item \textbf{After optimization}: 5.95ns (2$\times$ improvement, still 4.4$\times$ slower)
\item \textbf{Full benchmark}: 12\% regression across 80 patterns
\item \textbf{Alternations}: No patterns showed improvement, including target alternation cases
\end{itemize}

\paragraph{Root Cause Analysis}
The fundamental limitation is \textbf{execution model mismatch}:

\textbf{Base CTRE (Specialized Code)}:
\begin{verbatim}
// Compiler generates pattern-specific code:
if (input[0] == 'T' && input[1] == 'o' && ...)
// Or: SIMD instructions (_mm256_cmpge_epu8)
// Cost: 0.4-0.5ns per character
\end{verbatim}

\textbf{BitNFA (Generic State Machine)}:
\begin{verbatim}
// Generic code for any pattern:
for (char c : input) {
    current = nfa.calculate_successors(current, c);
}
// Cost: 2ns per character (even after optimization)
\end{verbatim}

Generic state machine execution cannot match specialized code performance, regardless of compile-time optimizations. Hyperscan achieves different results through:
(1) multi-pattern matching (process 1000+ patterns in one pass),
(2) GB-scale inputs that amortize overhead, and
(3) hand-optimized assembly for state transitions.

CTRE's workload (single pattern, small inputs, template specialization) favors the base evaluation model.

\paragraph{Decision}
BitNFA remains in the codebase (\textasciitilde2000 LOC, fully functional) but automatic dispatch is disabled. The implementation serves as validation that technique applicability depends critically on workload characteristics, not just algorithmic correctness.

\subsubsection{SIMD Character Class Matching}

Character class patterns \texttt{[a-z]}, \texttt{[0-9]} leverage SIMD comparison operations and constitute the primary performance optimization.

\paragraph{Range Matching}
For single contiguous range \texttt{[a-z]}:
\begin{verbatim}
__m256i data = _mm256_loadu_si256(ptr);
__m256i ge_min = _mm256_cmpgt_epi8(data, min-1);  // > 'a'-1
__m256i le_max = _mm256_cmpgt_epi8(max+1, data);  // < 'z'+1
__m256i in_range = _mm256_and_si256(ge_min, le_max);
\end{verbatim}

Processes 32 bytes per iteration on AVX2, 16 bytes on SSE2. The implementation includes 64-byte chunk processing with interleaved operations to maximize instruction-level parallelism.

\paragraph{Multi-Range and Negated Classes}
Multiple ranges \texttt{[a-zA-Z0-9]} combine via OR operations:
\begin{verbatim}
result = in_range1 | in_range2 | in_range3;
\end{verbatim}

Negated classes \texttt{[\^{}a-z]} apply NOT after range check:
\begin{verbatim}
result = or(lt_min, gt_max);  // Outside [min,max]
\end{verbatim}

\paragraph{Repetition Handling}
For quantified patterns \texttt{[a-z]+}, \texttt{a*}:
\begin{itemize}
\item SIMD scans 32-64 bytes per iteration while matches succeed
\item Tracks match length via pointer arithmetic
\item Handles minimum/maximum bounds (\texttt{\{m,n\}}) through iteration counts
\item Early exit on first mismatch minimizes wasted work
\end{itemize}

Input length thresholds prevent overhead on short strings: SIMD activates only for $\geq$16 bytes, ensuring scalar fallback for inputs below amortization threshold.

\paragraph{Performance Characteristics}
Empirical results show SIMD repetition matching achieves:
\begin{itemize}
\item 40-45$\times$ speedup on large repetitions (\texttt{[a-z]*} with 256-512 byte inputs)
\item 15-25$\times$ speedup on medium repetitions (64-128 byte inputs)
\item Near-baseline (8-10$\times$) on small repetitions (16-32 byte inputs)
\end{itemize}

The variation reflects fixed SIMD setup overhead amortizing over input length, consistent with theoretical expectations.

\subsubsection{Zero-Overhead Abstraction}

The implementation ensures patterns unsuitable for SIMD incur no performance penalty.

\paragraph{Compile-Time Elimination}
\texttt{if constexpr} removes inapplicable code paths:
\begin{verbatim}
if constexpr (can_use_simd<RE>) {
    // SIMD path (only instantiated if applicable)
} else {
    // Template path
}
\end{verbatim}

Assembly analysis confirms identical instruction sequences for patterns with and without SIMD infrastructure:
\begin{itemize}
\item Pattern \texttt{a+} without SIMD headers: 3-5 instructions
\item Pattern \texttt{a+} with SIMD headers: 3-5 instructions (identical)
\item Verification: \texttt{objdump -d} comparison shows byte-exact equivalence
\end{itemize}

\paragraph{Runtime Threshold Checks}
SIMD paths include length guards that compile to single branches with high prediction accuracy:
\begin{verbatim}
if constexpr (can_use_simd<RE>) {
    if (length >= 16) {  // Predictable: most inputs small
        // SIMD execution
    }
}
// Scalar fallback (common path)
\end{verbatim}

Modern branch predictors achieve $>$95\% accuracy on these guards, minimizing overhead.

\paragraph{Pattern-Specific Specialisation}
Each pattern instantiates only required code paths:
\begin{itemize}
\item \texttt{a+}: SIMD repeat only (no BitNFA, no literal extraction)
\item \texttt{(foo|bar)test}: Template evaluation + literal extraction
\item \texttt{[a-z]+}: SIMD range scan only
\end{itemize}

Template metaprogramming ensures each pattern compiles to minimal, specialized code without unused infrastructure.

\subsubsection{Architecture Trade-offs}

\paragraph{Compile-Time vs Runtime Overhead}
Graph analysis (Glushkov NFA construction, dominator/region analysis) occurs during C++ compilation. This increases compilation time (estimated 10-30\% for complex patterns) but produces optimal runtime dispatch with zero analysis overhead. The trade-off favors runtime performance in production deployments where compilation occurs once.

\paragraph{Code Size vs Performance}
SIMD paths increase binary size:
\begin{itemize}
\item Per-pattern overhead: 200-500 bytes for SIMD-enabled patterns
\item System-wide overhead: \textasciitilde5-10\% for typical applications
\item Mitigation: \texttt{if constexpr} guards prevent instantiation for unsuitable patterns
\end{itemize}

Binary size growth is localized to patterns that achieve corresponding performance gains, maintaining favorable cost-benefit ratio.

\paragraph{Portability}
SIMD intrinsics are architecture-specific. The implementation provides:
\begin{itemize}
\item x86-64: SSE2 (baseline), AVX2 (when available)
\item ARM: NEON support paths (not implemented, future work)
\item Portable fallback: Scalar evaluation for all platforms
\item Runtime detection: \texttt{\_\_SSE2\_\_}, \texttt{\_\_AVX2\_\_} compile-time macros
\end{itemize}

Patterns compile and execute correctly on all platforms; SIMD acceleration activates where hardware supports it, degrading gracefully to scalar evaluation otherwise.

\subsubsection{Implementation Summary}

The SIMD-accelerated matching model integrates vectorised operations into CTRE through:
\begin{enumerate}
\item Compile-time pattern classification (character class, repetition, complex)
\item Graph-based literal extraction (dominator and region analysis on Glushkov NFA)
\item Selective SIMD dispatch for character classes and repetitions
\item Zero-overhead abstraction ensuring unsuitable patterns incur no penalty
\item Scalar fallback for portability and short inputs
\end{enumerate}

Empirical evaluation across 80 test patterns demonstrates:
\begin{itemize}
\item \textbf{Average speedup}: 11.1$\times$ over baseline CTRE
\item \textbf{Peak performance}: 43.6$\times$ on optimal patterns (\texttt{[a-z]*} with 256+ byte inputs)
\item \textbf{Baseline preservation}: Patterns unsuitable for SIMD show 1.0-1.1$\times$ (no regression)
\item \textbf{Portability}: Correct execution on all tested platforms (x86-64, compilation verified)
\end{itemize}

This architecture preserves CTRE's compile-time guarantee (patterns validated at compilation) while exploiting modern CPU capabilities where applicable. The BitNFA investigation demonstrates that optimization technique selection must consider execution model characteristics, not merely algorithmic properties---a result with broader implications for high-performance pattern matching library design.

