\subsection{SIMD-Accelerated Matching Implementation}
\label{sec:simd-model}

This section describes the vectorisation strategies investigated and the rationale for their selective application. The implementation maintains CTRE's compile-time analysis model while integrating runtime vectorisation where empirically beneficial.

\subsubsection{Pattern Analysis and Execution Path Selection}

The matching pipeline employs compile-time pattern analysis to determine execution strategy. Analysis extracts pattern characteristics without runtime overhead via \texttt{constexpr} evaluation and \texttt{if constexpr} dispatch.

Pattern classification identifies:
\begin{itemize}
\item Character classes (\texttt{[a-z]}, \texttt{[0-9]}): Candidates for SIMD range comparison
\item Quantified repetitions (\texttt{a+}, \texttt{[a-z]*}): Candidates for vectorised bulk processing
\item Alternations (\texttt{(A|B|C)}): Evaluated for bit-parallel NFA representation
\item Complex constructs (backreferences, lookahead): Require template evaluation
\end{itemize}

Dispatch occurs via \texttt{if constexpr}, eliminating inapplicable paths at compile-time. Assembly verification confirms that including analysis infrastructure does not affect code generation for patterns that do not utilize it.

\subsubsection{Graph-Based Literal Extraction}

The implementation constructs a Glushkov NFA from the pattern AST at compile-time to identify required literals for prefiltering.

\paragraph{Glushkov NFA Construction}
The construction maps each regex position to an NFA state, computing:
\begin{itemize}
\item $\mathit{FIRST}(P)$: States reachable at pattern start
\item $\mathit{LAST}(P)$: States reachable at pattern end  
\item $\mathit{FOLLOW}(P, i)$: States reachable after position $i$
\end{itemize}

The resulting graph structure is \texttt{constexpr}, enabling compile-time traversal.

\paragraph{Dominator Analysis}
A state $v$ is a \emph{dominator} if all paths from start to accept traverse $v$. The algorithm tests each state by computing reachability with that state removed. Consecutive dominator positions with concrete characters form extractable literals (e.g., ``test'' from \texttt{(foo|bar)test}).

\paragraph{Region Analysis}
When dominator analysis yields no literals, region analysis partitions the NFA into alternation branches and identifies common suffixes. This fallback provides coverage for patterns with structural alternations.

\paragraph{Prefiltering Integration}
Extracted literals enable early rejection:
\begin{verbatim}
if constexpr (has_prefilter_literal<RE>) {
    constexpr auto literal = prefilter_literal<RE>;
    if constexpr (literal.length >= 2) {
        if (!scan_for_literal(begin, end, literal))
            return not_matched;
    }
}
\end{verbatim}

The scan employs a naive loop; the compiler vectorises it when beneficial. Analysis occurs entirely at compile-time; patterns without extractable literals incur no code path instantiation.

\subsubsection{BitNFA: Implementation and Evaluation}

Alternation patterns were hypothesized to benefit from bit-parallel NFA evaluation following Hyperscan's architecture~\cite{wang2019hyperscan}. A complete implementation was developed and empirically evaluated.

\paragraph{Architecture}
The BitNFA represents active states as bitmasks, with transitions computed via table lookups and bitwise operations:
\begin{verbatim}
StateMask current = nfa.get_initial_state();
for (char c : input) {
    current = nfa.calculate_successors(current, c);
}
return nfa.has_accept(current);
\end{verbatim}

NFA construction occurs at compile-time (\texttt{static constexpr}), eliminating construction overhead.

\paragraph{Optimization Attempts}
Initial implementation exhibited overhead relative to template evaluation. Optimization focused on the \texttt{calculate\_successors} hot path:
\begin{itemize}
\item Exception handling: Replaced linear state iteration with bit-scan operations (\texttt{\_\_builtin\_ctzll})
\item Branch elimination: Tested removing intermediate checks for small inputs
\item Pointer arithmetic: Replaced iterators with raw pointers for tighter code generation
\end{itemize}

The bit-scan optimization yielded factor-of-two improvement in micro-benchmarks.

\paragraph{Empirical Results and Analysis}
Systematic benchmarking across diverse patterns revealed BitNFA unsuitable for CTRE's workload. The fundamental limitation is \emph{execution model mismatch}:

\textbf{Template Evaluation}: Compiler generates pattern-specific code (direct comparisons, specialized SIMD operations). Per-character cost dominated by data-dependent operations.

\textbf{BitNFA Evaluation}: Generic state machine executing independent of pattern. Per-character cost dominated by state transition logic (reachability table lookups, bitmask operations), which persists regardless of optimization.

This differs from Hyperscan's deployment context:
\begin{itemize}
\item \textbf{Multi-pattern matching}: Hyperscan processes hundreds to thousands of patterns simultaneously; CTRE evaluates one pattern per invocation
\item \textbf{Input scale}: Hyperscan targets GB-scale streaming where setup overhead amortizes; tested workload comprises byte-to-KB scale match operations
\item \textbf{Optimization metric}: Hyperscan optimizes throughput (GB/s); CTRE optimizes latency (ns per operation)
\end{itemize}

Based on empirical evidence, BitNFA dispatch was disabled. The implementation remains in the codebase, demonstrating that technique applicability depends on workload characteristics, not solely algorithmic properties.

\subsubsection{SIMD Character Class Matching}

Character class repetitions (\texttt{[a-z]+}, \texttt{[0-9]*}) constitute the primary vectorisation target.

\paragraph{Range Comparison Strategy}
For contiguous ranges \texttt{[a-z]}, AVX2 comparison operations test 32 bytes simultaneously:
\begin{verbatim}
__m256i data = _mm256_loadu_si256(ptr);
__m256i ge_min = _mm256_cmpgt_epi8(data, min-1);
__m256i le_max = _mm256_cmpgt_epi8(max+1, data);
__m256i in_range = _mm256_and_si256(ge_min, le_max);
\end{verbatim}

The implementation processes 64-byte chunks where possible, with interleaved loads and comparisons to maximize instruction-level parallelism.

\paragraph{Multi-Range Handling}
Patterns with multiple disjoint ranges (\texttt{[a-zA-Z0-9]}) combine per-range results via bitwise OR. Negated classes (\texttt{[\^{}a-z]}) test for values outside the range via \texttt{lt\_min OR gt\_max}.

\paragraph{Repetition Strategy}
Quantified patterns (\texttt{+}, \texttt{*}, \texttt{\{m,n\}}) scan input in vectorised chunks:
\begin{itemize}
\item Load 32-byte chunk and apply character class test
\item If all bytes match: advance pointer by 32
\item If mismatch detected: identify position via \texttt{\_\_builtin\_ctz} on result bitmask
\item Repeat until input exhausted or maximum bound reached
\end{itemize}

\paragraph{Threshold Selection}
SIMD paths activate only for inputs exceeding 16 bytes. Below this threshold, setup overhead (vector loads, mask operations) exceeds scalar evaluation cost. The threshold was determined empirically by testing transition points across input sizes.

\subsubsection{Compile-Time Analysis, Runtime Execution}

The architecture separates compile-time analysis from runtime execution to maintain zero-overhead abstraction.

\paragraph{Compile-Time Phase}
Pattern parsing, NFA construction, dominator analysis, and dispatch selection occur during C++ compilation via template metaprogramming and \texttt{constexpr} evaluation. This increases compilation time but eliminates runtime analysis overhead.

\paragraph{Runtime Phase}
Only the selected execution path instantiates. Patterns unsuitable for SIMD follow the original template evaluation path with no additional code generated. The compiler's dead code elimination ensures unused infrastructure does not appear in final binaries.

\paragraph{Verification Methodology}
Zero-overhead claims were verified via assembly analysis:
\begin{itemize}
\item Generate object code with and without SIMD infrastructure headers
\item Compare instruction sequences via \texttt{objdump -d}
\item Confirm byte-exact equivalence for patterns not using SIMD paths
\end{itemize}

This verification confirms that compile-time analysis imposes no runtime penalty.

\subsubsection{Implementation Trade-offs}

\paragraph{Execution Model Selection}
The investigation of BitNFA versus template specialization demonstrates a fundamental trade-off: generic state machines provide uniform handling of diverse patterns but cannot match the performance of pattern-specific generated code. Template metaprogramming achieves specialization at the cost of increased compilation complexity and binary size per unique pattern.

\paragraph{Compile-Time Cost}
Pattern analysis, particularly Glushkov NFA construction and dominator analysis, increases compilation time. Complex patterns with many states or deep nesting exhibit longer compilation. This cost is incurred once per pattern; runtime performance is unaffected.

\paragraph{Binary Size}
SIMD-enabled patterns instantiate vectorised code paths, increasing per-pattern binary footprint. The \texttt{if constexpr} guards limit this growth to patterns demonstrating empirical benefit. Patterns unsuitable for SIMD do not instantiate unused code paths.

\paragraph{Portability}
SIMD intrinsics require architecture-specific implementations (SSE2, AVX2 for x86-64). The implementation provides compile-time detection and scalar fallback, ensuring correct execution on all platforms with performance acceleration where hardware supports it.

\subsubsection{Summary}

The SIMD-accelerated matching implementation integrates vectorisation into CTRE's template-based evaluation through:
\begin{enumerate}
\item Compile-time pattern classification via type traits and graph analysis
\item Selective SIMD dispatch for character classes and repetitions based on pattern structure
\item Graph-based literal extraction (dominator and region analysis) for prefiltering
\item Empirical evaluation of alternative approaches (BitNFA) with data-driven selection
\item Zero-overhead abstraction ensuring unsuitable patterns incur no penalty
\end{enumerate}

The implementation demonstrates that optimization technique selection must account for execution context---workload characteristics, input scale, and execution model---rather than algorithmic properties alone. This finding informed the selective application of Hyperscan-inspired techniques to CTRE's distinct deployment scenario.

